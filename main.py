import streamlit as st
import litstudy
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import numpy as np
import seaborn as sns
import pandas as pd
import streamlit.components.v1 as components
from wordcloud import WordCloud
import os

# --- CONFIGURARE PAGINÄ‚ ---
st.set_page_config(
    page_title="LitStudy Pro - AnalizÄƒ BibliometricÄƒ",
    layout="wide",
    page_icon="ğŸ“š"
)

# --- CSS CUSTOM ---
st.markdown("""
<style>
    .main { background-color: #f9f9f9; }
    h1 { color: #2c3e50; }
    .stButton>button { width: 100%; }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ“š LitStudy Pro: AnalizÄƒ BibliometricÄƒ AvansatÄƒ")
st.markdown("### Instrument pentru analiza automatÄƒ a literaturii È™tiinÈ›ifice")

# --- SIDEBAR: DATA LOADING ---
st.sidebar.header("1. SursÄƒ Date")
sursa_date = st.sidebar.radio("Metoda de import:", ("CÄƒutare Live (DBLP)", "FiÈ™ier Local"))

# IniÈ›ializÄƒm session_state pentru a nu pierde datele la refresh (filtrare)
if 'docs' not in st.session_state:
    st.session_state['docs'] = []

# --- FUNCÈšIE DE NORMALIZARE (FIX PENTRU TOATE FORMATELE) ---
def normalize_documents(new_docs):
    """
    AceastÄƒ funcÈ›ie reparÄƒ datele lipsÄƒ din obiectele litstudy,
    indiferent dacÄƒ vin din CSV, BIB sau RIS.
    """
    count_fixed = 0
    for new_doc in new_docs:
        # 1. FIX SOURCE (Jurnal/ConferinÈ›Äƒ)
        # DacÄƒ 'source' lipseÈ™te, Ã®ncercÄƒm sÄƒ Ã®l gÄƒsim Ã®n alte cÃ¢mpuri standard BibTeX/RIS
        if not hasattr(new_doc, 'source') or not new_doc.source or str(new_doc.source).lower() == 'nan':
            new_source = None
            
            # Ordinea de prioritate pentru a gÄƒsi sursa:
            if hasattr(new_doc, 'journal') and new_doc.journal:
                new_source = new_doc.journal
            elif hasattr(new_doc, 'booktitle') and new_doc.booktitle:
                new_source = new_doc.booktitle
            elif hasattr(new_doc, 'publisher') and new_doc.publisher:
                new_source = new_doc.publisher
            
            # AplicÄƒm sursa gÄƒsitÄƒ
            if new_source:
                new_doc.source = str(new_source)
                count_fixed += 1
            else:
                new_doc.source = "Unknown" # Ca sÄƒ nu crape graficul

    return new_docs, count_fixed

# LOGICA DE ÃNCÄ‚RCARE
new_docs = []

if 'regenerate_word_cloud' not in st.session_state:
    st.session_state['regenerate_word_cloud'] = False
regenerate_word_cloud = st.session_state['regenerate_word_cloud'] 

if sursa_date == "CÄƒutare Live (DBLP)":
    query = st.sidebar.text_input("Cuvinte cheie", value="Machine Learning")
    limit_docs = st.sidebar.slider("Nr. maxim articole", 100, 500, 100, step=100)
    
    if st.sidebar.button("ğŸ” CautÄƒ pe DBLP"):
        with st.spinner('Se descarcÄƒ datele de pe DBLP...'):
            try:
                new_docs = litstudy.search_dblp(query, limit=limit_docs)
                new_docs, _ = normalize_documents(new_docs)
                regenerate_word_cloud = True
                st.session_state['docs'] = new_docs
                st.success(f"GÄƒsite: {len(new_docs)} articole.")
            except Exception as e:
                st.error(f"Eroare: {e}")

else:
    uploaded_file = st.sidebar.file_uploader(
        "ÃncarcÄƒ fiÈ™ier (BibTeX, RIS, CSV)", 
        type=["bib", "ris", "csv"])
    if uploaded_file:
        temp_name = f"temp_{uploaded_file.name}"
        with open(temp_name, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        try:
            with st.spinner('Se proceseazÄƒ fiÈ™ierul...'):
                if temp_name.endswith(".csv"):
                    # 1. CITIM CU PANDAS PENTRU A REPARA DATELE
                    df_temp = pd.read_csv(temp_name)
                    
                    # Redenumim 'link' -> 'doi' pentru Litstudy
                    if 'link' in df_temp.columns and 'doi' not in df_temp.columns:
                        df_temp.rename(columns={'link': 'doi'}, inplace=True)
                    
                    # SalvÄƒm CSV-ul temporar corectat
                    df_temp.to_csv(temp_name, index=False)
                    
                    # ÃncÄƒrcÄƒm documentele de bazÄƒ
                    docs = litstudy.load_csv(temp_name)
                    
                    # --- FIX CRITIC PENTRU SURSE ---
                    # Litstudy ignorÄƒ coloana 'source' dacÄƒ nu e standard. O injectÄƒm manual.
                    if 'source' in df_temp.columns:
                        for i, doc in enumerate(docs):
                            if i < len(df_temp):
                                val = df_temp.iloc[i]['source']
                                # Ne asigurÄƒm cÄƒ e text valid
                                if pd.isna(val) or str(val).lower() == 'nan':
                                    doc.source = "Unknown"
                                else:
                                    doc.source = str(val)
                elif temp_name.endswith(".bib"):
                    docs = litstudy.load_bibtex(temp_name)
                elif temp_name.endswith(".ris"):
                    docs = litstudy.load_ris(temp_name)

                # --- APLICÄ‚M NORMALIZAREA PENTRU TOATE ---
                docs, fixed_count = normalize_documents(docs) 
                st.session_state['docs'] = docs
                st.sidebar.success(f"FiÈ™ier procesat: {len(docs)} articole")

                if fixed_count > 0:
                    st.sidebar.info(f"ğŸ› ï¸ S-au normalizat sursele pentru {fixed_count} articole.")
        except Exception as e:
            st.sidebar.error(f"Eroare fiÈ™ier: {e}")

# PreluÄƒm documentele din memorie
docs = st.session_state['docs']

# --- SISTEM DE FILTRARE ---
filtered_docs = docs
if docs:
    st.sidebar.markdown("---")
    st.sidebar.header("2. Filtrare Rezultate")
    
    # A. Filtru Ani
    years = [d.publication_year for d in docs if d.publication_year is not None]
    if years:
        min_y, max_y = int(min(years)), int(max(years))
        def slider_change_callback():
            st.session_state['regenerate_word_cloud'] = True
        sel_years = st.sidebar.slider("ğŸ“… Interval Ani", min_y, max_y, (min_y, max_y), key='my_slider', on_change=slider_change_callback)
        filtered_docs = [d for d in filtered_docs if d.publication_year and sel_years[0] <= d.publication_year <= sel_years[1]]

    # B. Filtru SursÄƒ (Jurnal)
    sources = list(set([d.source for d in docs if hasattr(d, 'source') and d.source]))
    if sources:
        sel_source = st.sidebar.multiselect("ğŸ“– Filtru Jurnal/ConferinÈ›Äƒ", sources)
        if sel_source:
            filtered_docs = [d for d in filtered_docs if hasattr(d, 'source') and d.source in sel_source]

    st.sidebar.info(f"Se analizeazÄƒ: **{len(filtered_docs)}** / {len(docs)} articole")

# --- INTERFAÈšA PRINCIPALÄ‚ ---
if filtered_docs:
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ“Š Dashboard Statistici", 
        "ğŸ§  Topic Modeling (NLP)", 
        "ğŸ•¸ï¸ ReÈ›ele", 
        "ğŸ“¥ Export Date"
    ])

    # === TAB 1: STATISTICI ===
    with tab1:
        st.subheader("Privire de ansamblu")
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**PublicaÈ›ii pe ani**")
            fig1 = plt.figure(figsize=(8, 4))
            litstudy.plot_year_histogram(filtered_docs)
            st.pyplot(fig1, use_container_width=True)
            
        with col2:
            st.markdown("**Top Autori**")
            fig2 = plt.figure(figsize=(8, 4))
            litstudy.plot_author_histogram(filtered_docs, limit=10)
            st.pyplot(fig2, use_container_width=True)

        st.markdown("---")
        
        col3, col4 = st.columns(2)
        with col3:
            st.markdown("**Top Surse de Publicare**") 
            # --- COD MANUAL PENTRU GRAFIC SURSE ---
            # VizualizeazÄƒ 'Top Surse de Publicare' pentru a identifica nucleul de cercetare.
            # Interpretare:
            # 1. ObservÄƒm o distribuÈ›ie "Long Tail" specificÄƒ bibliometriei (Legea lui Bradford).
            # 2. UCI ML Repository dominÄƒ ca sursÄƒ de date primarÄƒ (Dataset Hub).
            # 3. PMLR È™i Springer reprezintÄƒ canalele academice (ConferinÈ›e & Jurnale).

            #"Pe axa OX avem 'Venues', adicÄƒ locurile unde au apÄƒrut lucrÄƒrile. Graficul nostru aratÄƒ o diversitate mare:
            #Avem surse de date (precum UCI Repository).
            #Avem conferinÈ›e de specialitate (PMLR).
            #È˜i avem mari edituri academice (Springer, CRC Press) care grupeazÄƒ mai multe jurnale sub aceeaÈ™i umbrelÄƒ."

            sources_list = []
            for d in filtered_docs:
                if hasattr(d, 'source') and d.source and str(d.source) != "nan":
                    sources_list.append(d.source)
                elif hasattr(d, 'publisher') and d.publisher:
                    sources_list.append(d.publisher)
            
            if len(sources_list) > 0:
                s_counts = pd.Series(sources_list).value_counts().head(10)
                
                fig3, ax = plt.subplots(figsize=(8, 4))
                s_counts.plot(kind='bar', ax=ax, color='#4682B4') 
                
                ax.set_ylabel("No. of documents") 
                ax.set_xlabel("") # Scoatem eticheta de jos ca sÄƒ fie mai curat
                
                # Rotim etichetele de jos pentru a se citi uÈ™or
                plt.xticks(rotation=45, ha='right')
                
                # AjustÄƒm marginile ca sÄƒ nu taie textul
                plt.tight_layout()
                
                st.pyplot(fig3, use_container_width=True)
            else:
                st.warning("Nu au fost gÄƒsite informaÈ›ii despre Jurnal/ConferinÈ›Äƒ Ã®n date.")

        with col4:
            st.markdown("**Word Cloud (Din Titluri)**")
            # Generare rapidÄƒ WordCloud
            text = " ".join([d.title for d in filtered_docs if d.title])
            if text:
                if 'wc' not in st.session_state:
                    st.session_state['wc'] = None
                wc = st.session_state['wc']
                if regenerate_word_cloud or wc is None:
                    wc = WordCloud(width=800, height=400, background_color='white').generate(text)
                    st.session_state['wc'] = wc
                    st.session_state['regenerate_word_cloud'] = False
                fig_wc = plt.figure(figsize=(8, 4))
                plt.imshow(wc, interpolation='bilinear')
                plt.axis("off")
                st.pyplot(fig_wc, use_container_width=True)

    # === TAB 2: TOPIC MODELING (Implementare "Low-Level" Scikit-Learn) ===
    with tab2:
        st.subheader("Detectare AutomatÄƒ a Subiectelor (NMF)")
        st.markdown("""
        > AceastÄƒ secÈ›iune implementeazÄƒ algoritmul **NMF (Non-negative Matrix Factorization)** folosind direct 
        > biblioteca *Scikit-Learn* pentru o precizie maximÄƒ È™i control total asupra datelor.
        """)

        if len(filtered_docs) < 10:
            st.warning("âš ï¸ Ai nevoie de cel puÈ›in 10 articole pentru a genera topicuri relevante.")
        else:
            col_settings, col_viz = st.columns([1, 3])
            
            with col_settings:
                st.markdown("**SetÄƒri Model**")
                num_topics = st.slider("NumÄƒr de topicuri", 3, 10, 5)
                run_nlp = st.button("ğŸš€ RuleazÄƒ Analiza")

            with col_viz:
                if run_nlp:
                    with st.spinner("Se proceseazÄƒ textul È™i se antreneazÄƒ modelul NMF..."):
                        try:
                            # 1. PREGÄ‚TIRE DATE (Extragem textul din obiectele LitStudy)
                            # CombinÄƒm titlul cu abstractul (dacÄƒ existÄƒ) pentru fiecare articol
                            text_data = []
                            for doc in filtered_docs:
                                content = doc.title
                                if hasattr(doc, 'abstract') and doc.abstract:
                                    content += " " + doc.abstract
                                text_data.append(content)

                            # 2. VECTORIZARE (TF-IDF)
                            # TransformÄƒm textul Ã®n numere, eliminÃ¢nd cuvintele comune (stop words)
                            tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')
                            tfidf = tfidf_vectorizer.fit_transform(text_data)
                            feature_names = tfidf_vectorizer.get_feature_names_out()

                            # 3. ANTRENARE MODEL NMF
                            nmf_model = NMF(n_components=num_topics, random_state=42, init='nndsvd')
                            nmf_model.fit(tfidf)
                            
                            st.success("AnalizÄƒ finalizatÄƒ cu succes (Scikit-Learn Backend)!")
                            st.markdown("### ğŸ§© Rezultate Identificate:")

                            # 4. EXTRAGERE È˜I AFIÈ˜ARE TOPICURI
                            for topic_idx, topic in enumerate(nmf_model.components_):
                                # LuÄƒm top 10 cuvinte cu cea mai mare greutate Ã®n topic
                                top_indices = topic.argsort()[:-11:-1]
                                top_words = [feature_names[i] for i in top_indices]
                                
                                with st.expander(f"Topic {topic_idx + 1}: {top_words[0].upper()}..."):
                                    st.write(f"**Cuvinte cheie:** {', '.join(top_words)}")
                                    # AfiÈ™Äƒm un mini grafic de bare pentru importanÈ›a cuvintelor (Bonus vizual)
                                    topic_df = pd.DataFrame({
                                        'CuvÃ¢nt': top_words,
                                        'ImportanÈ›Äƒ': topic[top_indices]
                                    })
                                    st.bar_chart(topic_df.set_index('CuvÃ¢nt'))

                        except Exception as e:
                            st.error(f"A apÄƒrut o eroare la procesare: {e}")
                            st.info("VerificÄƒ dacÄƒ articolele selectate au abstracte disponibile.")

    # === TAB 3: REÈšELE ===
    with tab3:
        st.subheader("ReÈ›ea de Co-autori")
        st.info("AceastÄƒ vizualizare aratÄƒ grupurile de cercetÄƒtori care colaboreazÄƒ frecvent.")
        
        try:
            net = litstudy.build_coauthor_network(filtered_docs)
            if net and len(net.nodes) > 0:
                html_file = "network.html"
                litstudy.plot_network(net, height="600px")
                
                # Hack pentru a citi fiÈ™ierul generat de litstudy
                # De obicei Ã®l salveazÄƒ ca 'citation.html' sau deschide temp
                if os.path.exists("citation.html"):
                    with open("citation.html", 'r', encoding='utf-8') as f:
                        html_src = f.read()
                    components.html(html_src, height=620, scrolling=True)
                else:
                    st.warning("ReÈ›eaua a fost generatÄƒ Ã®n fundal.")
            else:
                st.warning("Nu existÄƒ suficiente conexiuni pentru o reÈ›ea.")
        except Exception as e:
            st.error(f"Eroare reÈ›ea: {e}")

    # === TAB 4: DATE & EXPORT ===
    with tab4:
        st.subheader("Export Date")
        
        # Conversie la Pandas DataFrame
        data = []
        for d in filtered_docs:
            data.append({
                "Titlu": d.title,
                "An": d.publication_year,
                "Autori": ", ".join([a.name for a in d.authors]) if d.authors else "",
                "SursÄƒ": d.source if hasattr(d, 'source') else ""
            })
        
        df = pd.DataFrame(data)
        st.dataframe(df, use_container_width=True)
        
        st.markdown("### ğŸ“¥ DescarcÄƒ Raport")
        col_dl1, col_dl2 = st.columns(2)
        
        with col_dl1:
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                "ğŸ“„ DescarcÄƒ Tabel (CSV)",
                data=csv,
                file_name="litstudy_export.csv",
                mime="text/csv"
            )
        
        with col_dl2:
            st.info("ğŸ’¡ Pentru raportul PDF complet, faceÈ›i o capturÄƒ de ecran a Tab-ului 'Dashboard Statistici' È™i includeÈ›i-o Ã®n documentaÈ›ie.")

elif not docs:
    st.info("ğŸ‘ˆ Ãncepe prin a cÄƒuta un termen sau a Ã®ncÄƒrca un fiÈ™ier Ã®n meniul din stÃ¢nga.")